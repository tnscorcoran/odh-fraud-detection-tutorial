{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding pyspark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Credit Card Data to ODH Ceph-Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_endpoint_url = os.environ['ENDPOINT_URL']\n",
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "s3_bucket=\"TOMBUCKET\"\n",
    "\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id = s3_access_key,aws_secret_access_key = s3_secret_key, endpoint_url=s3_endpoint_url)\n",
    "\n",
    "result = s3.list_objects(Bucket=s3_bucket)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import boto3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve\n",
    "        \n",
    "print(\"Getting a spark session with the distributed spark cluster running on Openshift \")\n",
    "\n",
    "#Spark Session\n",
    "# spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "import socket\n",
    "spark_address = socket.gethostbyname(\"spark-cluster-opentlc-mgr.00odh\")\n",
    "spark_cluster_url = f\"spark://{spark_address}:7077\"\n",
    "\n",
    "print(spark_cluster_url)\n",
    "spark = SparkSession.builder.appName(\"odh-pyspark\").master(spark_cluster_url).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "print(spark.sparkContext.version)\n",
    "print(\"Spark Session Success\")\n",
    "\n",
    "#Set the Hadoop configurations to access Ceph S3\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\");\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key) \n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", 'false')\n",
    "\n",
    "print(\"Spark reading transactional data\")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(f\"s3a://{s3_bucket}/OPEN/uploaded/creditcard.csv\")\n",
    "\n",
    "print(\"Total number of credit card transaction rows: %d\" % df.count())\n",
    "### Check the total number of rows with fraud is detected\n",
    "print(\"Total number of rows with fraud\")\n",
    "print(df[(df['Class']==1)].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_endpoint_url= \"[ROOK CEPH URL - NO PROTOCOL]\"\n",
    "s3_endpoint_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Order the credit card transaction by transaction time\n",
    "df.orderBy(\"Time\")\n",
    "\n",
    "#number of rows in the dataset\n",
    "n_samples = df.count()\n",
    "print(n_samples)\n",
    "\n",
    "#Split into train and test\n",
    "train_size = 0.75\n",
    "\n",
    "train_limit = int(n_samples * train_size)\n",
    "df_train = df.limit(train_limit)     \n",
    "df_test = df.subtract(df_train) \n",
    "\n",
    "#Data Schema\n",
    "print(\"Original Data Schema\")\n",
    "df_test.printSchema()\n",
    "\n",
    "print('Number of train transactions: %s', df_train.count())\n",
    "print('Number of test  transactions: %s', df_test.count())\n",
    "\n",
    "#Define features and target variables for convenience.\n",
    "drop_time_class = ['_c0', 'Time', 'Class']\n",
    "drop_class=['Class']\n",
    "\n",
    "#Create Train Datasets\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "#Create Test Datasets\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "\n",
    "#Create a RondomForest Classifier mode\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "\n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(train_labels, train_pred):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    labels = list(train_labels['Class'].value_counts().index)\n",
    "    print(labels)\n",
    "\n",
    "    confusion = confusion_matrix(train_labels, train_pred, labels=labels)\n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "\n",
    "    ax.set_xticklabels(labels, rotation=90);\n",
    "    ax.set_yticklabels(labels);\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('predicted')    \n",
    "    plt.ylabel('true')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(target_train_pd['Class'].value_counts())\n",
    "\n",
    "_=plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_=plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "feat_imp = sorted(zip(features_train_pd.columns, model.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in feat_imp], [i[1] for i in feat_imp], 'p-')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target variables for convenience.\n",
    "## From the graph we only want seven important features V3,V4,V10,V11,V12,V14,V17\n",
    "drop_time_class = ['_c0', 'Time', 'Class','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "drop_class=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "features_test.printSchema()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=6, n_jobs=10, class_weight='balanced')\n",
    "                               \n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))\n",
    "  \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_ = plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = ['V3','V4','V10','V11','V12','V14','V17','Amount'])\n",
    "    print(model.predict(rowdf))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Rook/Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"model.pkl\")\n",
    "prefix='uploaded/'\n",
    "result = s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Disconnect from Spark\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenShift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o oc.tar.gz -L https://mirror.openshift.com/pub/openshift-v3/clients/4.0.22/linux/oc.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp oc ~/../bin/oc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login into Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc login -u opentlc-mgr -p 'r3dh4t1!' --insecure-skip-tls-verify [OPENSHIFT MASTER API URL]:6443\n",
    "oc project 00odh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model With Seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project 00odh\n",
    "oc create -n 00odh -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/mymodel.json\n",
    "oc get seldondeployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model in Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp jq ~/../bin/jq\n",
    "chmod 777 ~/../bin/jq\n",
    "export TOKENJSON=$(curl -XPOST -u oauth-key:oauth-secret [SELDON URL]/oauth/token -d 'grant_type=client_credentials')\n",
    "export TOKEN=$(echo $TOKENJSON | jq \".access_token\" -r)\n",
    "echo $TOKEN\n",
    "\n",
    "curl -v --header \"Authorization: Bearer $TOKEN\" [SELDON URL]/api/v0.1/predictions -d '{\"strData\": \"0.365194527642578,0.819750231339882,-0.5927999453145171,-0.619484351930421,-2.84752569239798,1.48432160780265,0.499518887687186,72.98\"}' -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the served model from python using the test dataframe\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Get the token\n",
    "post_data = {\"grant_type\": \"client_credentials\"}\n",
    "requestOauth = requests.post('[SELDON URL]/oauth/token', auth=('oauth-key', 'oauth-secret'), data=post_data, json={'grant_type=client_credentials'})\n",
    "\n",
    "data = requestOauth.json();\n",
    "print(data['access_token'])\n",
    "access_token = data['access_token']\n",
    "\n",
    "headers = {'Content-type': 'application/json', 'Authorization': 'Bearer {}'.format(access_token)}\n",
    "#Read the test dataframe and stream each row\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "#for index, row in features_test.toPandas().iterrows():\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    str1 = ','.join(str(e) for e in  data)\n",
    "    requestPrediction = requests.post('[SELDON URL]/api/v0.1/predictions', headers=headers, json={\"strData\": str1 })\n",
    "    predictionData = requestPrediction.json();\n",
    "    datafield = predictionData['data']\n",
    "    predictionArray = datafield['ndarray']\n",
    "    print(predictionArray[0])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project 00odh\n",
    "oc delete seldondeployments mymodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
